{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Knowledge Shaping (RL-KS)\n",
    "## Comprehensive Analysis and Results\n",
    "\n",
    "**Project Team:**\n",
    "- Charith Kapuluru\n",
    "- Bhargav Reddy Alimili\n",
    "- Karthik Saraf\n",
    "- Sreekanth Taduru\n",
    "- Himesh Chander Addiga\n",
    "\n",
    "This notebook presents the complete analysis of our experiments comparing baseline Reinforcement Learning (RL) with Reinforcement Learning with Knowledge Shaping (RL-KS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Problem Statement\n",
    "Reinforcement Learning is powerful but slow to learn new tasks from scratch. Transfer learning through knowledge shaping can accelerate learning by incorporating prior knowledge from related source tasks.\n",
    "\n",
    "### Approach\n",
    "We implemented:\n",
    "1. **Baseline Q-Learning**: Standard tabular Q-learning agent\n",
    "2. **RL-KS**: Q-learning enhanced with knowledge shaping from a source task\n",
    "\n",
    "### Hypothesis\n",
    "RL-KS should demonstrate faster convergence and/or better sample efficiency compared to baseline RL by leveraging prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experimental Setup\n",
    "\n",
    "### Environments\n",
    "1. **GridWorld**: Discrete navigation task (5×5 grid)\n",
    "   - Source task: Specific obstacle configuration\n",
    "   - Target task: Different obstacle configuration (similar structure)\n",
    "\n",
    "2. **CartPole**: Classic control task with discretized state space\n",
    "   - Source task: Coarse-grained discretization (4×4×4×4)\n",
    "   - Target task: Fine-grained discretization (6×6×6×6)\n",
    "\n",
    "### Hyperparameters\n",
    "- Learning rate (α): 0.1\n",
    "- Discount factor (γ): 0.99\n",
    "- Initial epsilon (ε): 1.0\n",
    "- Epsilon decay: 0.995\n",
    "- Minimum epsilon: 0.01\n",
    "- Knowledge shaping weight (λ): 0.5 (GridWorld), 0.3 (CartPole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GridWorld Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GridWorld results\n",
    "with open('results/gridworld_results.pkl', 'rb') as f:\n",
    "    gridworld_results = pickle.load(f)\n",
    "\n",
    "gw_training = gridworld_results['training_results']\n",
    "gw_eval = gridworld_results['eval_results']\n",
    "\n",
    "print(\"GridWorld Results Summary\")\n",
    "print(\"=\"*60)\n",
    "for name, results in gw_eval.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Average Reward: {results['avg_reward']:.2f} ± {results['std_reward']:.2f}\")\n",
    "    print(f\"  Success Rate: {results['success_rate']:.2%}\")\n",
    "    print(f\"  Average Episode Length: {results['avg_length']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display GridWorld comparison plot\n",
    "display(Image('results/gridworld_comparison.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld Analysis\n",
    "\n",
    "Both agents achieved similar final performance, with 100% success rate. The GridWorld task demonstrates that:\n",
    "- Knowledge transfer is effective for structurally similar tasks\n",
    "- Both agents converge to optimal policies\n",
    "- The task complexity is relatively low, allowing both approaches to succeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CartPole Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CartPole results\n",
    "with open('results/cartpole_results.pkl', 'rb') as f:\n",
    "    cartpole_results = pickle.load(f)\n",
    "\n",
    "cp_training = cartpole_results['training_results']\n",
    "cp_eval = cartpole_results['eval_results']\n",
    "\n",
    "print(\"CartPole Results Summary\")\n",
    "print(\"=\"*60)\n",
    "for name, results in cp_eval.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Average Reward: {results['avg_reward']:.2f} ± {results['std_reward']:.2f}\")\n",
    "    print(f\"  Success Rate: {results['success_rate']:.2%}\")\n",
    "    print(f\"  Average Episode Length: {results['avg_length']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display CartPole comparison plot\n",
    "display(Image('results/cartpole_comparison.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole Analysis\n",
    "\n",
    "In CartPole, baseline RL outperformed RL-KS. This reveals important insights:\n",
    "1. **Domain Mismatch**: Source and target tasks had different discretizations\n",
    "2. **Negative Transfer**: Prior knowledge from coarser discretization may have interfered\n",
    "3. **State Space Difference**: Different binning strategies led to incompatible Q-values\n",
    "\n",
    "This demonstrates that knowledge transfer requires careful consideration of task similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# GridWorld Learning Curves\n",
    "ax1 = axes[0, 0]\n",
    "for name, results in gw_training.items():\n",
    "    rewards = results['episode_rewards']\n",
    "    window = 20\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(smoothed, label=name, linewidth=2)\n",
    "ax1.set_title('GridWorld: Training Rewards', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Reward (smoothed)', fontsize=12)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# GridWorld Episode Lengths\n",
    "ax2 = axes[0, 1]\n",
    "for name, results in gw_training.items():\n",
    "    lengths = results['episode_lengths']\n",
    "    window = 20\n",
    "    smoothed = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "    ax2.plot(smoothed, label=name, linewidth=2)\n",
    "ax2.set_title('GridWorld: Episode Lengths', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Steps (smoothed)', fontsize=12)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# CartPole Learning Curves\n",
    "ax3 = axes[1, 0]\n",
    "for name, results in cp_training.items():\n",
    "    rewards = results['episode_rewards']\n",
    "    window = 50\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax3.plot(smoothed, label=name, linewidth=2)\n",
    "ax3.set_title('CartPole: Training Rewards', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Episode', fontsize=12)\n",
    "ax3.set_ylabel('Reward (smoothed)', fontsize=12)\n",
    "ax3.legend(fontsize=11)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# CartPole Episode Lengths\n",
    "ax4 = axes[1, 1]\n",
    "for name, results in cp_training.items():\n",
    "    lengths = results['episode_lengths']\n",
    "    window = 50\n",
    "    smoothed = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "    ax4.plot(smoothed, label=name, linewidth=2)\n",
    "ax4.set_title('CartPole: Episode Lengths', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Episode', fontsize=12)\n",
    "ax4.set_ylabel('Steps (smoothed)', fontsize=12)\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comprehensive_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Comprehensive comparison plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings\n",
    "\n",
    "### Successes\n",
    "1. **Implementation**: Successfully implemented both baseline Q-learning and RL-KS\n",
    "2. **GridWorld Performance**: Both approaches achieved optimal performance on GridWorld\n",
    "3. **Framework**: Created reusable framework for knowledge transfer experiments\n",
    "\n",
    "### Challenges\n",
    "1. **CartPole Transfer**: Knowledge transfer was less effective due to:\n",
    "   - Different state space discretizations\n",
    "   - Potential negative transfer from mismatched representations\n",
    "2. **Convergence Detection**: Simple threshold-based convergence metrics\n",
    "3. **Computational Constraints**: Limited to tabular Q-learning\n",
    "\n",
    "### Insights\n",
    "1. **Task Similarity Matters**: Successful knowledge transfer requires similar state/action representations\n",
    "2. **Discretization Impact**: State space discretization significantly affects transfer effectiveness\n",
    "3. **Shaping Weight Tuning**: The knowledge shaping weight λ requires careful tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Future Improvements\n",
    "\n",
    "1. **Better Transfer Methods**:\n",
    "   - State mapping functions between different discretizations\n",
    "   - Progressive neural networks for deep RL\n",
    "   - Meta-learning approaches\n",
    "\n",
    "2. **Advanced Algorithms**:\n",
    "   - Deep Q-Networks (DQN) for continuous state spaces\n",
    "   - Actor-Critic methods (A3C, PPO)\n",
    "   - Successor representations\n",
    "\n",
    "3. **Evaluation Metrics**:\n",
    "   - Sample efficiency curves\n",
    "   - Transfer learning metrics (jumpstart, asymptotic performance)\n",
    "   - Statistical significance testing\n",
    "\n",
    "4. **Additional Experiments**:\n",
    "   - More diverse source-target task pairs\n",
    "   - Multiple source tasks\n",
    "   - Ablation studies on shaping weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This project demonstrated the implementation and evaluation of Reinforcement Learning with Knowledge Shaping. While RL-KS showed promise in GridWorld with structurally similar tasks, it faced challenges in CartPole due to representation mismatches.\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Knowledge transfer can accelerate learning when source and target tasks are sufficiently similar\n",
    "- State representation compatibility is crucial for successful transfer\n",
    "- Careful hyperparameter tuning (especially shaping weight) is essential\n",
    "- Baseline RL remains robust when transfer assumptions don't hold\n",
    "\n",
    "This work provides a foundation for understanding knowledge transfer in RL and highlights important considerations for practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. References\n",
    "\n",
    "1. Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\n",
    "2. Taylor, M. E., & Stone, P. (2009). Transfer learning for reinforcement learning domains: A survey. JMLR.\n",
    "3. Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under reward transformations: Theory and application to reward shaping. ICML.\n",
    "4. Brys, T., et al. (2015). Reinforcement learning from demonstration through shaping. IJCAI.\n",
    "5. OpenAI Gymnasium Documentation: https://gymnasium.farama.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
